<!DOCTYPE html>
<html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="GFN: Advancing Underwater Image Enhancement with Swin Transformer and Adaptive Transformations">
  <meta property="og:title" content="Gated Fusion Network for Underwater Image Enhancement"/>
  <meta property="og:description" content="GFN integrates advanced feature fusion and context-aware adaptive transformations for superior underwater imaging."/>
  <meta property="og:image" content="static/images/GFN (1).png" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>GFN: Gated Fusion Network for Underwater Image Enhancement</title>
  <link rel="icon" type="image/x-icon" href="static/images/78357759.jpg">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Bulma CSS & Other Styles -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- JavaScript Libraries -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* Fix word-wrapping issue for section titles */
    h2.title {
      text-align: center;
      white-space: nowrap;
      word-break: normal;
    }

    /* Ensures text doesnâ€™t break into vertical letters */
    .content p {
      text-align: justify;
      word-wrap: break-word;
    }

    /* Prevents unintended stacking of elements */
    .column {
      padding: 15px;
    }

    /* Limits container width to prevent wrapping issues */
    .container.is-max-desktop {
      max-width: 1100px;
    }
  </style>
</head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> Gated Fusion Network with Reprogramming Transformer Refiners for Adaptive Underwater
 Image Dehazing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#" target="_blank">Lyes Saad Saoud</a>,</span>
            
  <a href="#" target="_blank">Irfan Hussain</a>
</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Khalifa University of Science and Technology, UAE<br>Preprint 2024
            </span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/<ARXIV_PAPER_ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/LyesSaadSaoud/GFN_dehazing" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="position: relative; max-width: 100%; text-align: center;">
        <img src="static/images/GFN (1).png" alt="GFN system illustration" style="width: 70%; height: auto; border-radius: 10px;">
      </div>
      <h2 class="subtitle has-text-centered">
        Architecture of the proposed underwater image dehazing model, showcasing the Gated Fusion Network (GFN) with
 Reprogramming Adaptive Transformation Units (RATUs) for context-aware dehazing.
      </h2>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
         Underwater image quality is degraded due to light absorption, scattering, and low illumination, which impacts visual clarity and usability. These problems hinder the effectiveness of the use of such images in areas including marine studies, robotics, and environmental assessment. Many current methods are not able to produce satisfactory solutions for a wide range of underwater conditions which hampers their utilization. To address these issues, we propose a novel Gated Fusion Network (GFN), which integrates Reprogramming Adaptive Transformation Units (RATUs) and a Swin Transformer backbone to perform multi-stage feature fusion. The Swin Transformer is capable of extracting high detailed and contextual information and thus generates the confidence maps which control the RATUs to perform context specific operations like white balancing, gamma correction and histogram equalization. A gated fusion mechanism then selectively combines these enhancements, ensuring robust noise reduction and superior visual quality across diverse underwater conditions.
Testing on real underwater datasets showed clear advantages over existing methods. The GFN improved PSNR by 3.1 dB compared to WaterNet on the EUVP dataset. We saw similar gains with ocean_ex and LSUI400 data, pushing the boundaries of what's possible in underwater image enhancement.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="position: relative; max-width: 100%; text-align: center;">
        <img src="static/images/RATU (1).png" alt="GFN system illustration" style="width: 70%; height: auto; border-radius: 10px;">
      </div>
      <h2 class="subtitle has-text-centered">
        The proposed Reprogramming Adaptive Transformation Units (RATUs) for context-aware enhancement. The yellow
 blocks indicate the new components introduced in our model
      </h2>
    </div>
  </div>
</section>


  <!-- Image Carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <!-- Image 1 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig5.png" alt="Comparative visual analysis of SOTA methods for LSUI400 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 5:</strong> A comparative visual analysis of SOTA methods for different images from the LSUI400 dataset. Left to right columns show the input image, compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 2 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig6.png" alt="Comparative visual analysis of SOTA methods for ocean_ex dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 6:</strong> A comparative visual analysis of SOTA methods for different images from the ocean_ex dataset. Left to right columns show the Input image, Ground Truth, and outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 3 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig7.png" alt="Comparative visual analysis of SOTA methods for UIEB100 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 7:</strong> A comparative visual analysis of SOTA methods for different images from the UIEB100 dataset. Left to right columns show the Input image, Ground Truth, and outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 4 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig8.png" alt="Comparative visual analysis of SOTA methods for UPoor200 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 8:</strong> A comparative visual analysis of SOTA methods for different images from the UPoor200 dataset. Left to right columns show the Input image compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 5 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig9.png" alt="Comparative visual analysis of SOTA methods for U45 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 9:</strong> A comparative visual analysis of SOTA methods for different images from the U45 dataset. Left to right columns show the Input image compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 6 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig10.png" alt="Comparative visual analysis of SOTA methods for RUIE_Color90 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 10:</strong> A comparative visual analysis of SOTA methods for different images from the RUIE_Color90 dataset. Left to right columns show the Input image compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 7 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig11.png" alt="Comparative visual analysis of SOTA methods for challenging-60 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 11:</strong> A comparative visual analysis of SOTA methods for different images from the challenging-60 dataset. Left to right columns show the Input image compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 8 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig12.png" alt="Comparative visual analysis of SOTA methods for KUMP dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 12:</strong> Comparative visual analysis of SOTA methods for selected images from the KUMP dataset. Left to right columns display the Input image and outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End Image Carousel -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{GFN2024,
  author = {Saad Saoud, Lyes et al.},
  title = {GFN: Gated Fusion Network for Underwater Image Enhancement},
  year = {2024},
  publisher = {Preprint},
  doi = {......},
  url = {https://arxiv.org/...}}
</code></pre>
  </div>
</section>


  <section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Before and After Dehazing</h2>
    <div class="columns is-multiline">
      <!-- First Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before1.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after11.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Second Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before2.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after22.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Third Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before3.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after33.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Fourth Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before4.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after44.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Include JuxtaposeJS Library -->
<link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
<script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>


<style>
  /* Define slightly smaller dimensions for each slider */
  .slider-container {
    width: 100%; /* Full width of its parent container */
    max-width: 450px; /* Reduced maximum width */
    height: 300px; /* Reduced height */
    margin: 10px auto; /* Center each slider with spacing */
    position: relative;
  }

  /* Ensure images fill their containers */
  .uniform-slider img {
    width: 100%;
    height: 100%;
    object-fit: cover; /* Preserve aspect ratio and fill container */
  }

  /* Grid layout for 2x2 matrix */
  .columns.is-multiline {
    display: grid;
    grid-template-columns: repeat(2, 1fr); /* Two equal-width columns */
    grid-gap: 20px; /* Space between grid items */
    justify-items: center; /* Center each grid item */
  }

  .column {
    display: flex;
    justify-content: center;
    align-items: center;
  }

  /* Make labels more prominent */
  .juxtapose .jx-label {
    font-size: 0px; /* Adjust label size for visibility */
  }
</style>
<footer class="footer">
  <div class="container">
    <p>This page provides supplementary materials for "<strong>GFN: Gated Fusion Network for Underwater Image Enhancement</strong>." Access the paper, dataset, and code repository for more details.</p>
  </div>
</footer>

</body>
</html> 
