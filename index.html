<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HuBot: A Biomimicking Mobile Robot for Non-Disruptive Bird Behavior Study</title>
  <link rel="icon" type="image/x-icon" href="static/images/78357759.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>


<body>

<!-- Title Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1">Gated Fusion Network with Reprogramming Transformer Refiners for Adaptive Underwater Image Dehazing</h1>
      <p class="is-size-5">Lyes Saad Saoud, Irfan Hussain</p>
      <p class="is-size-5">Khalifa University of Science and Technology, UAE | Preprint 2024</p>

      <div class="buttons is-centered">
        <a href="https://arxiv.org/pdf/<ARXIV_PAPER_ID>.pdf" class="button is-dark is-rounded">
          <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
        </a>
        <a href="https://github.com/LyesSaadSaoud/GFN_dehazing" class="button is-dark is-rounded">
          <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
        </a>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="static/images/GFN (1).png" alt="GFN system illustration" style="width: 70%; border-radius: 10px;">
      <h2 class="subtitle">Architecture of the proposed underwater image dehazing model.</h2>
    </div>
  </div>
</section>

<!-- Abstract Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Abstract</h2>
    <div class="content">
      <p>Underwater image quality is degraded due to light absorption, scattering, and low illumination, affecting applications in marine research, robotics, and environmental assessment. Many existing methods fail to address a wide range of underwater conditions effectively. To overcome these challenges, we propose the Gated Fusion Network (GFN), integrating Reprogramming Adaptive Transformation Units (RATUs) and a Swin Transformer backbone for multi-stage feature fusion.</p>

      <p>The Swin Transformer extracts fine details and generates confidence maps, guiding RATUs to perform adaptive transformations such as white balancing, gamma correction, and histogram equalization. A gated fusion mechanism selectively combines these enhancements, ensuring robust noise reduction and superior visual quality.</p>

      <p>Testing on real underwater datasets demonstrates significant improvements, with the GFN outperforming WaterNet by 3.1 dB PSNR on the EUVP dataset. Similar gains were observed on ocean_ex and LSUI400 datasets, setting new benchmarks for underwater image enhancement.</p>
    </div>
  </div>
</section>


<!-- End Image Carousel -->
<!-- Results Section -->
<!-- Image Carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <!-- Image 3 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image7.png" alt="Qualitative comparison of segmentation results" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 1:</strong> Qualitative comparison of segmentation results. This figure compares the segmentation performance of various models on sample images, highlighting differences in segmentation quality and accuracy. (a) Input image, (b) ground truth, (c) DeepLabV3, (d) LRASPP, (e) FCN, (f) Mask R-CNN, (g) YOLOv5-Seg, (h) YOLOv8-Seg, (i) Detectron2, (j) MobileSAM Baseline, and (k) YOLOv9+MobileSAM.
            </h2>
          </div>
        </div>

        <!-- Image 4 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image6.png" alt="Comparative visual illustration of state-of-the-art depth estimation methods" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 2:</strong> A comparative visual illustration of state-of-the-art depth estimation methods applied to sample images from the depth dataset. This comparison includes the local-depth estimation approach implemented in the Hubot alongside other baseline methods. Sample images span a range of lighting conditions for assessing their impact on depth-estimation performance.
            </h2>
          </div>
        </div>

        <!-- Image 5 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image5.png" alt="Processed images of Houbara bustards demonstrating detection and positioning" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 3:</strong> Processed images of Houbara bustards demonstrating detection and relative positioning within the field of view. The center of gravity of the image is marked with a blue cross, whereas the detected Houbara is highlighted with a red mask.
            </h2>
          </div>
        </div>

        <!-- Image 6 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image3.png" alt="Examples of HuBot’s movements across various environments" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 4:</strong> Examples of HuBot’s movements across various environments: Top row: Feeding motion sequence demonstrating biomimetic movements during feeding behaviors. Second row: Head left and right motion showcasing head-orientation adjustments, mimicking natural head movements. Third row: Movement on a flat surface illustrating stability and navigation on even terrain. Fourth row: Movement on a grass surface highlighting adaptability to grassy terrains. Bottom row: Movement on a granular surface depicting maneuverability on sandy and granular surfaces. These images demonstrate HuBot’s operational capabilities under different conditions, enhancing understanding of its observations with varied environments.
            </h2>
          </div>
        </div>

        <!-- Image 7 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image4.png" alt="HuBot’s preliminary trials with Houbara bustards in captivity" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 5:</strong> HuBot’s preliminary trials with Houbara bustards in captivity. Top row: interaction with Female Houbara bustard, reacting with avoidance and cautious behavior. Second row: Male performing sexual courtship behavior directed at the HuBot. Third row: Male exhibiting a cautious stance, closely observing HuBot’s movements without closely approaching and interacting. Bottom Row: Male performing behaviors similar to the first male, including courtship display behavior. These examples illustrate varied behavioral responses of captive Houbara bustards toward the HuBot, linked to sex, individual personality, and, for males, individual differences in sexual motivation.
            </h2>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End Image Carousel -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{GFN2024,
  author = {Saad Saoud, Lyes et al.},
  title = {GFN: Gated Fusion Network for Underwater Image Enhancement},
  year = {2024},
  publisher = {Preprint},
  doi = {......},
  url = {https://arxiv.org/...}}
</code></pre>
  </div>
</section>


  <section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Before and After Dehazing</h2>
    <div class="columns is-multiline">
      <!-- First Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before1.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after11.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Second Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before2.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after22.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Third Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before3.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after33.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Fourth Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before4.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after44.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Include JuxtaposeJS Library -->
<link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
<script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>


<style>
  /* Define slightly smaller dimensions for each slider */
  .slider-container {
    width: 100%; /* Full width of its parent container */
    max-width: 450px; /* Reduced maximum width */
    height: 300px; /* Reduced height */
    margin: 10px auto; /* Center each slider with spacing */
    position: relative;
  }

  /* Ensure images fill their containers */
  .uniform-slider img {
    width: 100%;
    height: 100%;
    object-fit: cover; /* Preserve aspect ratio and fill container */
  }

  /* Grid layout for 2x2 matrix */
  .columns.is-multiline {
    display: grid;
    grid-template-columns: repeat(2, 1fr); /* Two equal-width columns */
    grid-gap: 20px; /* Space between grid items */
    justify-items: center; /* Center each grid item */
  }

  .column {
    display: flex;
    justify-content: center;
    align-items: center;
  }

  /* Make labels more prominent */
  .juxtapose .jx-label {
    font-size: 0px; /* Adjust label size for visibility */
  }
</style>
<footer class="footer">
  <div class="container">
    <p>This page provides supplementary materials for "<strong>GFN: Gated Fusion Network for Underwater Image Enhancement</strong>." Access the paper, dataset, and code repository for more details.</p>
  </div>
</footer>

</body>
</html> 
