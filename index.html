<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="AI-Powered Coral Watch: Pattern Recognition for Automated Bleaching Detection">
  <meta property="og:title" content="Gated Fusion Network for Underwater Image Enhancement"/>
  <meta property="og:description" content="AI-Powered Coral Watch: Pattern Recognition for Automated Bleaching Detection."/>
  <meta property="og:image" content="static/images/GFN (1).png" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>AI-Powered Coral Watch: Pattern Recognition for Automated Bleaching Detection</title>
  <link rel="icon" type="image/x-icon" href="static/images/78357759.jpg">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Bulma CSS & Other Styles -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- JavaScript Libraries -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>

  <style>
    /* Prevents unwanted word wrapping on section titles */
    h2.title {
      display: inline-block;
      text-align: center;
      white-space: nowrap;
      word-break: normal;
      overflow-wrap: break-word;
    }

    /* Ensures paragraphs do not break into vertical letters */
    .content p {
      text-align: justify;
      word-wrap: break-word;
      overflow-wrap: break-word;
    }

    /* Ensures titles and text stay in a horizontal layout */
    .column {
      padding: 15px;
      display: flex;
      justify-content: center;
      align-items: center;
    }

    /* Limits container width to prevent excessive wrapping */
    .container.is-max-desktop {
      max-width: 1100px;
    }

    /* Carousel Styling */
    .carousel {
      width: 100%;
      max-width: 900px;
      margin: auto;
    }

    .carousel img {
      width: 100%;
      height: auto;
      border-radius: 10px;
    }
  </style>
</head>

<body>

<!-- Title Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1">AI-Powered Coral Watch: Pattern Recognition for Automated Bleaching Detection</h1>
      <p class="is-size-5">Lyes Saad Saoud and Irfan Hussain</p>
      <p class="is-size-5">Khalifa University of Science and Technology, UAE | Preprint 2025</p>

      <div class="buttons is-centered">
        <a href="https://arxiv.org/pdf/<ARXIV_PAPER_ID>.pdf" class="button is-dark is-rounded">
          <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
        </a>
        <a href="https://github.com/LyesSaadSaoud/GFN_dehazing" class="button is-dark is-rounded">
          <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
        </a>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="static/images/main_figure.png" alt="GFN system illustration" style="width: 70%; border-radius: 10px;">
      <h2 class="subtitle">Comparison of manual and AI-powered coral bleaching detection.
    The left panel illustrates the traditional diver-based Coral Watch methodology, where divers manually assess coral bleaching by visually comparing coral colors to a reference chart and recording observations on waterproof slates.
    The right panel presents the proposed AI-driven automated framework, integrating a Remotely Operated Vehicle (ROV) equipped with deep learning and vision-language models for large-scale, objective coral monitoring. The system utilizes YOLOv12 for coral detection, SAM, for instance, segmentation, and DehazeFormer for underwater image enhancement, addressing visibility challenges in real-world conditions. 
    To enable automated bleaching classification, the pipeline leverages GPT-4o as a Generative AI model, which interprets segmented coral regions, assigns Coral Watch classifications based on dominant colors and provides structured conservation insights. The model is validated using a dual dataset approach, combining synthetic and real-world underwater images from both controlled marine pools and open ocean environments.
    Key advantages of this AI-powered system include real-time depth estimation, cloud-based AI processing, enhanced classification accuracy, and scalability compared to traditional methods. By integrating pattern recognition, deep learning, and generative AI, the proposed approach offers a more efficient, consistent, and scalable alternative to diver-based coral bleaching assessments, significantly improving monitoring efforts for marine conservation..</h2>
    </div>
  </div>
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Before and After Dehazing</h2>
    <div class="columns is-multiline">
      <!-- First Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before1.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after11.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Second Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before2.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after22.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Third Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before3.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after33.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>

      <!-- Fourth Slider -->
      <div class="column">
        <div class="slider-container">
          <div class="juxtapose uniform-slider" data-startingposition="50%">
            <img src="static/images/before4.jpg" alt="Before Dehazing" data-label="Before">
            <img src="static/images/after44.jpg" alt="After Dehazing" data-label="After">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Include JuxtaposeJS Library -->
<link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
<script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>


<style>
  /* Define slightly smaller dimensions for each slider */
  .slider-container {
    width: 100%; /* Full width of its parent container */
    max-width: 450px; /* Reduced maximum width */
    height: 300px; /* Reduced height */
    margin: 10px auto; /* Center each slider with spacing */
    position: relative;
  }

  /* Ensure images fill their containers */
  .uniform-slider img {
    width: 100%;
    height: 100%;
    object-fit: cover; /* Preserve aspect ratio and fill container */
  }

  /* Grid layout for 2x2 matrix */
  .columns.is-multiline {
    display: grid;
    grid-template-columns: repeat(2, 1fr); /* Two equal-width columns */
    grid-gap: 20px; /* Space between grid items */
    justify-items: center; /* Center each grid item */
  }

  .column {
    display: flex;
    justify-content: center;
    align-items: center;
  }

  /* Make labels more prominent */
  .juxtapose .jx-label {
    font-size: 0px; /* Adjust label size for visibility */
  }
</style>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <p>
        <div class="content has-text-justified">
          <p>
        Coral bleaching detection is essential for marine conservation, traditionally relying on diver-based surveys using Coral Watch color charts. However, this manual process is subjective, labor-intensive, and lacks scalability. In this work, we propose an AI-powered, multi-stage framework that automates Coral Watch-based bleaching detection using deep learning, generative AI, and pattern recognition techniques.
Our approach integrates YOLOv12 for coral detection, the Segment Anything Model (SAM) for instance segmentation, and a GPT-based vision-language model for automated color classification aligned with Coral Watch standards. To address underwater visibility challenges, we introduce a dual dataset approach, leveraging both synthetic and real-world underwater images. We trained and benchmarked state-of-the-art dehazing models, including DehazeFormer, RAUNE-Net, WaterNet, and UT-UIE, and demonstrated their effectiveness in improving coral visibility and detection accuracy.
Furthermore, we employ Generative AI (GenAI) to enhance the automated assessment process by interpreting segmented coral regions, classifying their health status based on color distributions, and providing conservation recommendations in natural language. The impact of dehazing on coral detection was quantified using a fine-tuned YOLOv12 model, revealing significant improvements in mean Average Precision (mAP) and recall scores across different visibility conditions.
The system was validated on datasets collected in controlled marine pools and open ocean environments, confirming its robustness for real-world deployment. Additionally, we present a comparative evaluation between AI-driven classification and manual expert assessments, demonstrating a near-perfect alignment. Our results highlight the effectiveness of deep learning, generative AI, and image processing techniques in automating large-scale coral bleaching detection, offering a scalable, objective, and efficient alternative to traditional diver-based monitoring.
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="position: relative; max-width: 100%; text-align: center;">
        <img src="static/images/Proposed_framework.png" alt="GFN system illustration" style="width: 70%; height: auto; border-radius: 10px;">
      </div>
      <h2 class="subtitle has-text-centered">
        The proposed Reprogramming Adaptive Transformation Units (RATUs) for context-aware enhancement. The yellow
 blocks indicate the new components introduced in our model
      </h2>
    </div>
  </div>
</section>


  <!-- Image Carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <!-- Image 1 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig5.png" alt="Comparative visual analysis of SOTA methods for LSUI400 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 5:</strong> A comparative visual analysis of SOTA methods for different images from the LSUI400 dataset. Left to right columns show the input image, compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 2 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig6.png" alt="Comparative visual analysis of SOTA methods for ocean_ex dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 6:</strong> A comparative visual analysis of SOTA methods for different images from the ocean_ex dataset. Left to right columns show the Input image, Ground Truth, and outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 3 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig7.png" alt="Comparative visual analysis of SOTA methods for UIEB100 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 7:</strong> A comparative visual analysis of SOTA methods for different images from the UIEB100 dataset. Left to right columns show the Input image, Ground Truth, and outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 4 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig8.png" alt="Comparative visual analysis of SOTA methods for UPoor200 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 8:</strong> A comparative visual analysis of SOTA methods for different images from the UPoor200 dataset. Left to right columns show the Input image compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 5 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig9.png" alt="Comparative visual analysis of SOTA methods for U45 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 9:</strong> A comparative visual analysis of SOTA methods for different images from the U45 dataset. Left to right columns show the Input image compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 6 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig10.png" alt="Comparative visual analysis of SOTA methods for RUIE_Color90 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 10:</strong> A comparative visual analysis of SOTA methods for different images from the RUIE_Color90 dataset. Left to right columns show the Input image compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 7 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig11.png" alt="Comparative visual analysis of SOTA methods for challenging-60 dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 11:</strong> A comparative visual analysis of SOTA methods for different images from the challenging-60 dataset. Left to right columns show the Input image compared to outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

        <!-- Image 8 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig12.png" alt="Comparative visual analysis of SOTA methods for KUMP dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 12:</strong> Comparative visual analysis of SOTA methods for selected images from the KUMP dataset. Left to right columns display the Input image and outputs of models such as UGAN Fabbri et al. (2018), FUnIEGAN Islam et al. (2020b), Cycle-GAN Zhu et al. (2017), PUGAN Cong et al. (2023), RAUNE-Net Peng et al. (2024), UT-UIE Peng et al. (2023b), WaterNet Li et al. (2019b), UTM-UIE, and our models, including CNN-RATU, GFN-FTU, and GFN.
            </h2>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End Image Carousel -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{AI-CoralWatch,
  author = {Saad Saoud, Lyes et al.},
  title = {AI-Powered Coral Watch: Pattern Recognition for Automated Bleaching Detection},
  year = {2025},
  publisher = {Preprint},
  doi = {......},
  url = {https://arxiv.org/...}}
</code></pre>
  </div>
</section>


  
<footer class="footer">
  <div class="container">
    <p>This page provides supplementary materials for "<strong>AI-Powered Coral Watch: Pattern Recognition for Automated Bleaching Detection</strong>." Access the paper, dataset, and code repository for more details.</p>
  </div>
</footer>

</body>
</html> 
